# â˜€ï¸ Solar Data Discovery â€“ KAIM / 10 Academy Week 1

### ğŸ‘©ğŸ½â€ğŸ’» Author: Betelhem Kibret Getu  
**Program:** KAIM / 10 Academy â€“ Week 1  
**Challenge:** Solar Data Discovery  
**Repository:** [https://github.com/Bettey-k/solar-challenge-week1](https://github.com/Bettey-k/solar-challenge-week1)

---

## ğŸŒ Project Overview
This project focuses on **cleaning, profiling, and analyzing solar energy datasets** from three West African countries â€” **Benin**, **Sierra Leone**, and **Togo**.  
It was developed as part of **10 Academyâ€™s KAIM Week 1 Challenge** to demonstrate skills in:
- Data cleaning and preprocessing  
- Exploratory Data Analysis (EDA)  
- Python modularization and documentation  
- Version control and CI/CD automation

---

## ğŸ§© Objectives
1. Load, profile, and clean raw solar datasets.  
2. Handle missing values and remove outliers using Z-score filtering.  
3. Perform descriptive statistics and correlation analysis.  
4. Visualize solar patterns across time and environmental variables.  
5. Build reusable Python modules for data cleaning and analysis.  
6. Maintain version control best practices (branches, commits, CI tests).

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone and create environment
```bash
git clone https://github.com/Bettey-k/solar-challenge-week1.git
cd solar-challenge-week1
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run tests
pytest -q


You should see something like:

2 passed in 0.5s

4ï¸âƒ£ Launch Jupyter Notebooks
jupyter notebook notebooks/

5ï¸âƒ£ Optional: Run functions directly
python src/data_cleaning.py


or import in notebooks:

from src.data_cleaning import clean_solar_data

ğŸ§± Repository Structure
solar-challenge-week1/
â”‚
â”œâ”€â”€ .github/workflows/ci.yml        â†’ Continuous Integration (pytest + flake8)
â”œâ”€â”€ src/                            â†’ Modular code (cleaning + analysis)
â”œâ”€â”€ notebooks/                      â†’ EDA and visual analysis per country
â”œâ”€â”€ tests/                          â†’ Unit tests for validation
â”œâ”€â”€ scripts/                        â†’ Placeholder for future automation
â”œâ”€â”€ output/                         â†’ Generated figures and summaries
â”œâ”€â”€ data/                           â†’ Local data (ignored in .gitignore)
â”œâ”€â”€ requirements.txt                â†’ Project dependencies
â””â”€â”€ README.md                       â†’ Main documentation (this file)

ğŸ“Š Steps in the Project
Step 1. Environment & Git Setup

Created virtual environment and requirements file.

Added CI workflow in .github/workflows/ci.yml to verify builds.

Initialized repository with .gitignore and folder structure.

Step 2. Data Profiling & Cleaning

Loaded raw datasets from the data/ folder.

Used functions in src/data_cleaning.py to:

Replace missing values with median.

Remove outliers using Z-scores.

Validate column consistency.

Step 3. Exploratory Data Analysis (EDA)

Conducted analysis in separate notebooks:

benin_eda.ipynb

sierra_leone_eda.ipynb

togo_eda.ipynb

Visualized time-series, correlations, humidity-temperature relations, and cleaning impact.

Step 4. Modularization

Abstracted core cleaning and visualization functions into:

src/data_cleaning.py

src/data_analysis.py

Added detailed docstrings for clarity and reusability.

Step 5. Testing & CI

Added tests/test_cleaning.py to verify functions.

Enabled GitHub Actions to run automatic tests.

Step 6. Documentation

Added detailed README files across folders.

Ensured clear structure and instructions for reproducibility.
